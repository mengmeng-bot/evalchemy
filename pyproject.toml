[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.poetry]
name = "evalchemy"
version = "0.1.0"
description = "Evalchemy Evaluation Framework"
authors = ["Your Name <your.email@example.com>"]
readme = "README.md"
license = "MIT"  # add a placeholder or your actual license
packages = [{ include = "evalchemy" }]


[tool.poetry.dependencies]
python = ">=3.10"

# Core ML and Deep Learning
torch = "*"
torchvision = "*"
transformers = "*"
accelerate = "*"
peft = "*"
trl = "*"
bitsandbytes = "*"
sentencepiece = "*"
sentence-transformers = ">=2.2.2"
optimum = "1.12.0"

# Machine Learning and Scientific Computing
numpy = "*"
pandas = "*"
scipy = "*"
scikit-learn = "*"
faiss-cpu = "1.7.4"

# ML Experiment Tracking & Visualization
wandb = "*"
tensorboard = "*"
evaluate = "*"

# NLP & Text Processing
nltk = "*"
fasttext-wheel = "*"
codebleu = "*"
sacrebleu = "*"
langdetect = "*"
fuzzywuzzy = "*"
python-Levenshtein = "*"

# Data Processing & Parsing
datasets = "*"
protobuf = "*"
bs4 = "*"
lxml = "*"
fastavro = "*"
jsonpickle = "*"
hjson = "*"
msgpack = "*"

# AI Service Providers
openai = "*"
anthropic = "*"
cohere = "*"
google-generativeai = "*"
mistralai = "*"
reka-api = "*"
together = "*"
dashscope = "*"
fschat = { path = "eval/chat_benchmarks/MTBench", develop = true }

# Cloud & Storage
gcsfs = "*"
google-cloud-aiplatform = "*"
google-auth = "2.25.1"
boto3 = "*"
botocore = "*"
sagemaker = "*"
fsspec = "2024.6.1"
hf-transfer = "*"
huggingface_hub = { version = "*", extras = ["cli"] }

# Code Analysis & Development
black = "*"
tree-sitter-python = "*"
tree-sitter-java = "*"
tree_sitter = "*"
asttokens = "*"
mypy_extensions = "*"
pyext = { git = "https://github.com/penfever/PyExt" }

# Web & API
fastapi = ">=0.101.0"
uvicorn = ">=0.23.0"
aiohttp = { version = ">=3.8", extras = ["speedups"] }
httpx = "*"
httpx-sse = "*"
requests = ">=2.28"
websocket = "*"
aiofiles = "*"
bespokelabs-curator = ">=0.1.16.0"

# Database
sqlalchemy = "*"
psycopg = { version = "*", extras = ["binary"] }
psycopg2-binary = "*"
sqlitedict = "*"

# Utilities
tqdm = "*"
pqdm = "*"
python-box = "*"
fire = "*"
tiktoken = "*"
ray = { version = "*", extras = ["default"] }
backoff = ">=2.2"
docker-pycreds = "*"
sentry-sdk = "*"
loguru = ">=0.7"
python-dotenv = "*"
shortuuid = "*"
tenacity = "*"
prettytable = "*"
portalocker = "*"
tabulate = "*"
colorama = "*"
termcolor = "*"
distro = "*"
typing_inspect = "*"
py-cpuinfo = "*"
jiter = "*"
patsy = "*"
joblib = "*"
threadpoolctl = "*"
jmespath = "*"
immutabledict = "*"

# BigCodeBench
appdirs = ">=1.4.4"
multipledispatch = ">=0.6.0"
tempdir = ">=0.7.1"
tree_sitter_languages = ">=1.10.2"
wget = ">=3.2"

# SWE-bench
swebench = ">=3.0.4"

# LM Eval
lm-eval = { git = "https://github.com/EtashGuha/lm-evaluation-harness" }



[tool.poetry.urls]
Homepage = "https://github.com/mlfoundations/evalchemy"
Repository = "https://github.com/mlfoundations/evalchemy.git"

[tool.poetry.group.dev.dependencies]
pytest = "*"
black = "*"
isort = "*"
flake8 = "*"
pre-commit = "*"



[tool.hatch.build.targets.wheel]
packages = ["evalchemy", "database"]

[tool.hatch.metadata]
allow-direct-references = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]

[tool.black]
line-length = 120

[tool.isort]
profile = "black"
multi_line_output = 3

[tool.setuptools]
packages = ["evalchemy"]

[tool.setuptools.package-dir]
evalchemy = "evalchemy"
